gr[j,t-1] <- sum(nnew)/sum(n)
n <- nnew
}
if(min(nstore) < threshold) ext_ind[j] = 1
if(X==1) extr_event[j]=1
}
ln_lambda_s <- mean(log(gr))      # the stochastic population growth rate
Lambda_s <- exp(ln_lambda_s)      # the stochastic population growth rate on the
nb_extr_event<-sum(extr_event)
# non-logged scale
ext_prob <- mean(ext_ind)         # the pseudo-extinction probability
ln_lambda_s
Lambda_s
ext_prob # the overall extinction probability
nb_extr_event # number of extinction events over the model run
23/500
32/500
# --------------------------------------------------
# Vulture Metapopulation pseudo-extinction probability
# --------------------------------------------------
# useful link http://www.mbr-pwrc.usgs.gov/workshops/uf2016/
# clean everything first
rm(list=ls())
# load required packages
library(popbio)
library(diagram)
#--------------------------------------------
# PARAMETERS
#--------------------------------------------
# fecundity calculation, (Gauthier & Lebreton (2004) Population models for Greater Snow Geese)
bp <- 0.8 # breeding propensity
cs <- 1 # clutch size
hs <- 0.76 # hatching success
fs <- 0.6 # fledging success
f1 <- bp * (cs/2) * hs * fs # divide by 2 to get females only
# survival
s0 <- 0.42 # first year survival # this value should probably be modified to account for
# lower adult survival in KZN
s1Kr <- 0.82 # juvenile survival Kruger
s2Kr <- 0.89 # subadult survival Kruger
s3Kr <- 1.0 # adult survival Kruger
#--------------------------------------------
# KRUGER PRE-BREEDING CENSUS
#--------------------------------------------
MKrpre <- c(0,0,0,0,s0*f1,
s1Kr,0,0,0,0,
0,s1Kr,0,0,0,
0,0,s2Kr,0,0,
0,0,0,s2Kr,s3Kr)
MKrpre <- matrix ((MKrpre), ncol=5, byrow = TRUE)
lambda(MKrpre)
#--------------------------------------------
# KZN SURVIVAL RATES
#--------------------------------------------
s1Kz <-  0.86 # juvenile survival KZN
s2Kz <- 0.51 # subadult survival KZN
s3Kz <- 0.57 # adult survival KZN
#--------------------------------------------
#--------------------------------------------
# KZN PRE-BREEDING CENSUS
#--------------------------------------------
MKZpre <- c(0,0,0,0,s0*f1,
s1Kz,0,0,0,0,
0,s1Kz,0,0,0,
0,0,s2Kz,0,0,
0,0,0,s2Kz,s3Kz)
MKZpre <- matrix ((MKZpre), ncol=5, byrow = TRUE)
lambda(MKZpre)
#--------------------------------------------
# MODELLING EXTINCTION PROBABILITIES
#--------------------------------------------
# Specify the number of simulations, time steps for each simulation, and the
# pseudo-extinction threshold
sims <- 500
tspan <- 50
threshold <- 20
# Define demographic parameters that do not vary over time
f1 <- 0.1824 # fecundity
gb <- 0.01 # migration rates from Kruger to KZN
bg <- 0.01 # migration rates from KZN to Kruger
s0 <- 0.42 # first year survival
s1Kz <- 0.86 # juvenile survival KZN
s2Kz <- 0.51 # Subadult survival KZN
s3Kz <- 0.57 # adult survival KZN
s1Kr <- 0.82 # juvenile survival Kruger
# Storage place for per time step growth rates for eventual calculation of
# the stochastic growth rate
gr <- matrix(0,sims,tspan-1)
# Storage for indicators on each simulation determining whether or not the
# population ever dropped below the pseudo-extinction threshold.
ext_ind <- matrix(0,sims,1)
extr_event <- matrix(0,sims,1)
for (j in 1:sims){
# Define vector of initial abundance
# juvenile < 2 years = 9%, immature, 3-5 years = 24%, adult > 5 years = 67%
n <- c(54,54,144,144,804,100,100,100,100,100)
nstore <- matrix(0,tspan,1) # temporary storage of time-specific abundance
nstore[1] <- sum(n)
for(t in 2:tspan){
X <- rbinom(1, 1, 1/15) # probabilty that poisoning occurs, here it's once ever 15 years
s2Kr <- 0.89 - (1/2*0.89*X) # Kruger subadult survival halves under poisoning
s3Kr <- 1 - (1/2*1*X) # Kruger adult survival halves under poisoning
# The following megamatrix matrix - A - has the Kruger matrix in top left
# and KZN matrix in bottom right
# the diagonals are include the probability of emigration/immigration between sites
# that's why it's called a megamatrix
# take a look at the simpler matrices above constructed separately for each site
# to see where these values come from
A <- matrix(c(
0,  0,  0,  0,  s0*f1,  0,  0,  0,  0, 0,
s1Kr*(1-gb), 0, 0, 0, 0, s1Kz*bg, 0, 0, 0, 0,
0, s1Kr*(1-gb), 0, 0, 0, 0, s1Kz*bg, 0, 0, 0,
0, 0, s2Kr*(1-gb), 0, 0, 0, 0, s2Kz*bg, 0, 0,
0, 0, 0, s2Kr*(1-gb), s3Kr*(1-gb), 0, 0, 0, s2Kz*bg, s3Kz*bg,
0, 0, 0, 0, 0, 0, 0, 0, 0, s0*f1,
s1Kr*gb, 0, 0, 0, 0, s1Kz*(1-bg), 0, 0, 0, 0,
0, s1Kr*gb, 0, 0, 0, 0, s1Kz*(1-bg), 0, 0, 0,
0, 0, s2Kr*gb, 0, 0, 0, 0, s2Kz*(1-bg), 0, 0,
0, 0, 0, s2Kr*gb, s3Kr*gb, 0, 0, 0, s2Kz*(1-bg), s3Kz*(1-bg)), nrow = 10, byrow = TRUE)
nnew <- A%*%n
nstore[t] <- sum(nnew)
gr[j,t-1] <- sum(nnew)/sum(n)
n <- nnew
}
if(min(nstore) < threshold) ext_ind[j] = 1
if(X==1) extr_event[j]=1
}
ln_lambda_s <- mean(log(gr))      # the stochastic population growth rate
Lambda_s <- exp(ln_lambda_s)      # the stochastic population growth rate on the
nb_extr_event<-sum(extr_event)
# non-logged scale
ext_prob <- mean(ext_ind)         # the pseudo-extinction probability
ln_lambda_s
Lambda_s
ext_prob # the overall extinction probability
nb_extr_event # number of extinction events over the model run
36/500
# --------------------------------------------------
# Vulture Metapopulation pseudo-extinction probability
# --------------------------------------------------
# useful link http://www.mbr-pwrc.usgs.gov/workshops/uf2016/
# clean everything first
rm(list=ls())
# load required packages
library(popbio)
library(diagram)
#--------------------------------------------
# PARAMETERS
#--------------------------------------------
# fecundity calculation, (Gauthier & Lebreton (2004) Population models for Greater Snow Geese)
bp <- 0.8 # breeding propensity
cs <- 1 # clutch size
hs <- 0.76 # hatching success
fs <- 0.6 # fledging success
f1 <- bp * (cs/2) * hs * fs # divide by 2 to get females only
# survival
s0 <- 0.42 # first year survival # this value should probably be modified to account for
# lower adult survival in KZN
s1Kr <- 0.82 # juvenile survival Kruger
s2Kr <- 0.89 # subadult survival Kruger
s3Kr <- 1.0 # adult survival Kruger
#--------------------------------------------
# KRUGER PRE-BREEDING CENSUS
#--------------------------------------------
MKrpre <- c(0,0,0,0,s0*f1,
s1Kr,0,0,0,0,
0,s1Kr,0,0,0,
0,0,s2Kr,0,0,
0,0,0,s2Kr,s3Kr)
MKrpre <- matrix ((MKrpre), ncol=5, byrow = TRUE)
lambda(MKrpre)
#--------------------------------------------
# KZN SURVIVAL RATES
#--------------------------------------------
s1Kz <-  0.86 # juvenile survival KZN
s2Kz <- 0.51 # subadult survival KZN
s3Kz <- 0.57 # adult survival KZN
#--------------------------------------------
#--------------------------------------------
# KZN PRE-BREEDING CENSUS
#--------------------------------------------
MKZpre <- c(0,0,0,0,s0*f1,
s1Kz,0,0,0,0,
0,s1Kz,0,0,0,
0,0,s2Kz,0,0,
0,0,0,s2Kz,s3Kz)
MKZpre <- matrix ((MKZpre), ncol=5, byrow = TRUE)
lambda(MKZpre)
#--------------------------------------------
# MODELLING EXTINCTION PROBABILITIES
#--------------------------------------------
# Specify the number of simulations, time steps for each simulation, and the
# pseudo-extinction threshold
sims <- 500
tspan <- 50
threshold <- 20
# Define demographic parameters that do not vary over time
f1 <- 0.1824 # fecundity
gb <- 0.01 # migration rates from Kruger to KZN
bg <- 0.01 # migration rates from KZN to Kruger
s0 <- 0.42 # first year survival
s1Kz <- 0.86 # juvenile survival KZN
s2Kz <- 0.51 # Subadult survival KZN
s3Kz <- 0.57 # adult survival KZN
s1Kr <- 0.82 # juvenile survival Kruger
# Storage place for per time step growth rates for eventual calculation of
# the stochastic growth rate
gr <- matrix(0,sims,tspan-1)
# Storage for indicators on each simulation determining whether or not the
# population ever dropped below the pseudo-extinction threshold.
ext_ind <- matrix(0,sims,1)
extr_event <- matrix(0,sims,1)
for (j in 1:sims){
# Define vector of initial abundance
# juvenile < 2 years = 9%, immature, 3-5 years = 24%, adult > 5 years = 67%
n <- c(54,54,144,144,804,100,100,100,100,100)
nstore <- matrix(0,tspan,1) # temporary storage of time-specific abundance
nstore[1] <- sum(n)
for(t in 2:tspan){
X <- rbinom(1, 1, 1/15) # probabilty that poisoning occurs, here it's once ever 15 years
s2Kr <- 0.89 - (1/2*0.89*X) # Kruger subadult survival halves under poisoning
s3Kr <- 1 - (1/2*1*X) # Kruger adult survival halves under poisoning
# The following megamatrix matrix - A - has the Kruger matrix in top left
# and KZN matrix in bottom right
# the diagonals are include the probability of emigration/immigration between sites
# that's why it's called a megamatrix
# take a look at the simpler matrices above constructed separately for each site
# to see where these values come from
A <- matrix(c(
0,  0,  0,  0,  s0*f1,  0,  0,  0,  0, 0,
s1Kr*(1-gb), 0, 0, 0, 0, s1Kz*bg, 0, 0, 0, 0,
0, s1Kr*(1-gb), 0, 0, 0, 0, s1Kz*bg, 0, 0, 0,
0, 0, s2Kr*(1-gb), 0, 0, 0, 0, s2Kz*bg, 0, 0,
0, 0, 0, s2Kr*(1-gb), s3Kr*(1-gb), 0, 0, 0, s2Kz*bg, s3Kz*bg,
0, 0, 0, 0, 0, 0, 0, 0, 0, s0*f1,
s1Kr*gb, 0, 0, 0, 0, s1Kz*(1-bg), 0, 0, 0, 0,
0, s1Kr*gb, 0, 0, 0, 0, s1Kz*(1-bg), 0, 0, 0,
0, 0, s2Kr*gb, 0, 0, 0, 0, s2Kz*(1-bg), 0, 0,
0, 0, 0, s2Kr*gb, s3Kr*gb, 0, 0, 0, s2Kz*(1-bg), s3Kz*(1-bg)), nrow = 10, byrow = TRUE)
nnew <- A%*%n
nstore[t] <- sum(nnew)
gr[j,t-1] <- sum(nnew)/sum(n)
n <- nnew
}
if(min(nstore) < threshold) ext_ind[j] = 1
if(X==1) extr_event[j]=1
}
ln_lambda_s <- mean(log(gr))      # the stochastic population growth rate
Lambda_s <- exp(ln_lambda_s)      # the stochastic population growth rate on the
nb_extr_event<-sum(extr_event)
# non-logged scale
ext_prob <- mean(ext_ind)         # the pseudo-extinction probability
ln_lambda_s
Lambda_s
ext_prob # the overall extinction probability
nb_extr_event # number of extinction events over the model run
rm(list=ls())
library(momentuHMM)
library(crawl)
URL <- paste0("https://www.datarepository.movebank.org/bitstream/handle/",
"10255/move.373/Elliptical%20Time-Density%20Model%20%28Wall%",
"20et%20al.%202014%29%20African%20Elephant%20Dataset%20%",
"28Source-Save%20the%20Elephants%29.csv")
rawData <- read.csv(url(URL))
# select and rename relevant columns
rawData <- rawData[,c(11,3,4,5,6)]
colnames(rawData) <- c("ID","time","lon","lat","temp")
# only keep first track
rawData <- subset(rawData,ID==unique(ID)[1])
# convert times from factors to POSIX
rawData$time <- as.POSIXct(rawData$time,tz="GMT")
# project to UTM coordinates using package rgdal
library(rgdal)
llcoord <- SpatialPoints(rawData[,3:4],
proj4string=CRS("+proj=longlat +datum=WGS84"))
utmcoord <- spTransform(llcoord,CRS("+proj=utm +zone=30 ellps=WGS84"))
# add UTM locations to data frame
rawData$x <- attr(utmcoord,"coords")[,1]
rawData$y <- attr(utmcoord,"coords")[,2]
# initial parameters for crawl fit
inits <- list(a = c(rawData$x[1],0,rawData$y[1],0),
P = diag(c(1, 1, 1, 1)))
#P = diag(c(5000^2, 10*3600^2, 5000^2, 10*3600^2)))
# fit crawl model
crwOut <- crawlWrap(obsData=rawData, timeStep="hour", initial.state=inits,
theta=c(4,-10), fixPar=c(NA,NA))
# select and rename relevant columns
rawData <- rawData[,c(11,3,4,5,6)]
colnames(rawData) <- c("ID","time","lon","lat","temp")
# only keep first track
rawData <- subset(rawData,ID==unique(ID)[1])
# convert times from factors to POSIX
rawData$time <- as.POSIXct(rawData$time,tz="GMT")
# project to UTM coordinates using package rgdal
library(rgdal)
llcoord <- SpatialPoints(rawData[,3:4],
proj4string=CRS("+proj=longlat +datum=WGS84"))
utmcoord <- spTransform(llcoord,CRS("+proj=utm +zone=30 ellps=WGS84"))
# add UTM locations to data frame
rawData$x <- attr(utmcoord,"coords")[,1]
rawData$y <- attr(utmcoord,"coords")[,2]
# initial parameters for crawl fit
inits <- list(a = c(rawData$x[1],0,rawData$y[1],0),
P = diag(c(1, 1, 1, 1)))
#P = diag(c(5000^2, 10*3600^2, 5000^2, 10*3600^2)))
# fit crawl model
crwOut <- crawlWrap(obsData=rawData, timeStep="hour", initial.state=inits,
theta=c(4,-10), fixPar=c(NA,NA))
initial.dry <- list(
a1.x=c(b30414_Seq1$X[1],0),
a1.y=c(b30414_Seq1$Y[1],0),
P1.x=diag(c(1,1)),
P1.y=diag(c(1,1))
)
initial.dry <- list(
a1.x=c(2,0),
a1.y=c(4,0),
P1.x=diag(c(1,1)),
P1.y=diag(c(1,1))
)
initial.dry
inits <- list(a = c(rawData$x[1],0,rawData$y[1],0),
P = diag(c(1, 0, 0, 1)))
inits
initial.dry
inits <- list(a = c(rawData$x[1],0,rawData$y[1],0),
P1.x=diag(c(1,1)),
P1.y=diag(c(1,1)))
inits
# select and rename relevant columns
rawData <- rawData[,c(11,3,4,5,6)]
colnames(rawData) <- c("ID","time","lon","lat","temp")
# only keep first track
rawData <- subset(rawData,ID==unique(ID)[1])
# convert times from factors to POSIX
rawData$time <- as.POSIXct(rawData$time,tz="GMT")
# project to UTM coordinates using package rgdal
library(rgdal)
llcoord <- SpatialPoints(rawData[,3:4],
proj4string=CRS("+proj=longlat +datum=WGS84"))
utmcoord <- spTransform(llcoord,CRS("+proj=utm +zone=30 ellps=WGS84"))
# add UTM locations to data frame
rawData$x <- attr(utmcoord,"coords")[,1]
rawData$y <- attr(utmcoord,"coords")[,2]
# initial parameters for crawl fit
inits <- list(a = c(rawData$x[1],0,rawData$y[1],0),
P1.x=diag(c(1,1)),
P1.y=diag(c(1,1)))
#P = diag(c(5000^2, 10*3600^2, 5000^2, 10*3600^2)))
# fit crawl model
crwOut <- crawlWrap(obsData=rawData, timeStep="hour", initial.state=inits,
theta=c(4,-10), fixPar=c(NA,NA))
P = diag(c(1, 1, 1, 1))
P
initial.dry
#head(rawData)
#which(rawData$tdiff > 60)
#rawData[1552,]
# select and rename relevant columns
rawData <- rawData[,c(11,3,4,5,6)]
colnames(rawData) <- c("ID","time","lon","lat","temp")
# only keep first track
rawData <- subset(rawData,ID==unique(ID)[1])
# convert times from factors to POSIX
rawData$time <- as.POSIXct(rawData$time,tz="GMT")
# project to UTM coordinates using package rgdal
library(rgdal)
llcoord <- SpatialPoints(rawData[,3:4],
proj4string=CRS("+proj=longlat +datum=WGS84"))
utmcoord <- spTransform(llcoord,CRS("+proj=utm +zone=30 ellps=WGS84"))
# add UTM locations to data frame
rawData$x <- attr(utmcoord,"coords")[,1]
rawData$y <- attr(utmcoord,"coords")[,2]
# initial parameters for crawl fit
inits <- list(a = c(rawData$x[1],0,rawData$y[1],0),
P = diag(c(1, 1, 1, 1)))
#P = diag(c(5000^2, 10*3600^2, 5000^2, 10*3600^2)))
# fit crawl model
crwOut <- crawlWrap(obsData=rawData, timeStep="hour", initial.state=inits,
theta=c(4,-10), fixPar=c(NA,NA))
rawData <- rawData[,c(11,3,4,5,6)]
rm(list=ls())
library(devtools)
install_github("pablobarbera/Rfacebook/Rfacebook")
install.packages("httpuv")
library(devtools)
install_github("pablobarbera/Rfacebook/Rfacebook")
library(Rfacebook)
token <- 'EAACEdEose0cBAOq7SJU5skpwwTZCiSGxSipHlHxOg68pWGs1AX5kIAxuIr7EqZAL3iOKazRRIre5UfzoDFerTftirWKOdoaV8rGgG9CLPyqWOtohTYj1j708cuO2N0jQmmpdIbXVxy5FZCZA5mVLfhHDxfO7cdTaEjiBFbrODt5NPw8ByJndZC92Buiqdgm4ZD'
me <- getUsers("me", token, private_info=TRUE)
me$name # my name
# [1] "Pablo Barberá"
me$hometown # my hometown
# https://www.facebook.com/WWF/
page <- getPage("WWF", token, n = 5000, since='2016/01/01', until='2016/12/31')
page
page <- getPage("WWF", token, n = 5000)
getLikes('WWF', token = token)[1,]
getLikes('me', token = token)[1,]
getLikes('me', token = token)
getLikes('WWF', token = token)
?getLikes
getLikes('WWF', token)
library(devtools)
install_github("pablobarbera/Rfacebook/Rfacebook")
library(Rfacebook)
token <- 'EAACEdEose0cBALnfN4HRfBFA5co8XGH0Rxqaoh9ZAQ7poZAO1pwwIJIkvKAghL7GXFFEplKSxzRZA5U8FIQycQ6hb2A1FuyFRq4uALEIvPtdbmmpCqDGRQUIj01oPKQiVzXpCXllbAaO9lRNH7muMtC3QQJsgkvinVDAoYWGXCvz3w68eHUa8c1hZCtJl38ZD'
me <- getUsers("me", token, private_info=TRUE)
me$name # my name
# [1] "Pablo Barberá"
me$hometown # my hometown
getLikes('WWF', token = token)
getLikes('bbcearth', token = token)
getLikes("bbcearth", token = token)
# Changepoint Analysis in R
rm(list=ls())
# https://rpubs.com/richkt/269908
library(pageviews)
# library(changepoint)
library(dplyr)
# library(data.table)
# USA airdates
# 18th Feb, 25th Feb, 4th March, 11th March, 18th March, 25th March
# UK airdates
# 6th Nov, 13th Nov, 20th Nov, 27th Nov, 4th Dec, 11th Dec
# load in the data which is a vector of species names
setwd("C:\\Users\\akane\\Desktop\\Science\\Manuscripts\\Documentary analysis\\Documentary-analysis")
data<-read.csv("mentionedNames.csv", header = TRUE, sep = ",")
# articles <- c("Rugby_union","Football") # test using these articles
articles <- data$name
# the dates before airing
before <- function(x){article_pageviews(project = "en.wikipedia", article = x
, start = as.Date('2016-05-01'), end = as.Date("2016-10-31")
, user_type = "user", platform = c("mobile-web"))
}
# the dates after airing
after <- function(x){article_pageviews(project = "en.wikipedia", article = x
, start = as.Date('2017-05-01'), end = as.Date("2017-10-31")
, user_type = "user", platform = c("mobile-web"))
}
# collect the wiki hits for before
outputBefore <-articles %>% before
# collect the wiki hits for after
outputAfter <-articles %>% after
# split the dataset by article and find the median wiki hits for each before the date
splitArticlesBefore<-with(outputBefore, tapply(views, outputBefore$article, median))
splitArticlesBefore<-data.frame(splitArticlesBefore)
# split the dataset by article and find the median wiki hits for each
splitArticlesAfter<-with(outputAfter, tapply(views, outputAfter$article, median))
splitArticlesAfter<-data.frame(outputAfter)
splitArticlesAfter
outputBefore
# split the dataset by article and find the median wiki hits for each before the date
splitArticlesBefore<-with(outputBefore, tapply(views, outputBefore$article, median))
splitArticlesBefore
splitArticlesBefore<-data.frame(splitArticlesBefore)
splitArticlesBefore
# split the dataset by article and find the median wiki hits for each
splitArticlesAfter<-with(outputAfter, tapply(views, outputAfter$article, median))
splitArticlesAfter<-data.frame(outputAfter)
splitArticlesAfter
outputAfter
# split the dataset by article and find the median wiki hits for each
splitArticlesAfter<-with(outputAfter, tapply(views, outputAfter$article, median))
splitArticlesAfter
splitArticlesAfter<-data.frame(outputAfter)
splitArticlesAfter
# split the dataset by article and find the median wiki hits for each
splitArticlesAfter<-with(outputAfter, tapply(views, outputAfter$article, median))
splitArticlesAfter
splitArticlesAfter<-data.frame(splitArticlesAfter)
splitArticlesAfter
names(splitArticlesAfter)
combinedHits <- merge(splitArticlesBefore,splitArticlesAfter)
combinedHits
head(splitArticlesAfter)
splitArticlesAfter$splitArticlesAfter
# split the dataset by article and find the median wiki hits for each before the date
splitArticlesBefore<-with(outputBefore, tapply(views, outputBefore$article, median))
class(splitArticlesBefore)
splitArticlesBefore<-data.frame(splitArticlesBefore)
splitArticlesBefore
# collect the wiki hits for before
outputBefore <-articles %>% before
# collect the wiki hits for after
outputAfter <-articles %>% after
# split the dataset by article and find the median wiki hits for each before the date
splitArticlesBefore<-with(outputBefore, tapply(views, outputBefore$article, median))
#splitArticlesBefore<-data.frame(splitArticlesBefore)
# split the dataset by article and find the median wiki hits for each
splitArticlesAfter<-with(outputAfter, tapply(views, outputAfter$article, median))
#splitArticlesAfter<-data.frame(splitArticlesAfter)
class(splitArticlesAfter)
# combine the arrays
combinedHits = cbind(splitArticlesBefore, splitArticlesAfter)
combinedHits
# which one is bigger? Before or after?
combinedHits$change<-ifelse(combinedHits$splitArticlesAfter>combinedHits$splitArticlesBefore,1,0)
combinedHits$splitArticlesAfter
names(combinedHits)
head(combinedHits)
combinedHits<-data.frame(combinedHits)
combinedHits
names(combinedHits)
combinedHits$splitArticlesBefore
combinedHits$splitArticlesAfter
# which one is bigger? Before or after?
combinedHits$change<-ifelse(combinedHits$splitArticlesAfter>combinedHits$splitArticlesBefore,1,0)
combinedHits
sum(combinedHits$change)
sum(combinedHits$change) /length(combinedHits$change)
(417.5/509.0) * 100
